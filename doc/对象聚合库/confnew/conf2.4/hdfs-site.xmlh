<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<property>
		<name>dfs.nameservices</name>
		<value>ficusMars</value>
	</property>

	<property>
		<name>dfs.ha.namenodes.ficusMars</name>
		<value>nn1,nn2</value>
	</property>

	<property>
		<name>dfs.namenode.rpc-address.ficusMars.nn1</name>
		<value>nnobjaof.mars.grid.sina.com.cn:9000</value>
	</property>

	<property>
		<name>dfs.namenode.http-address.ficusMars.nn1</name>
		<value>nnobjaof.mars.grid.sina.com.cn:50070</value>
	</property>

	<property>
		<name>dfs.namenode.rpc-address.ficusMars.nn2</name>
		<value>h112098.mars.grid.sina.com.cn:9000</value>
	</property>

	<property>
		<name>dfs.namenode.http-address.ficusMars.nn2</name>
		<value>h112098.mars.grid.sina.com.cn:50070</value>
	</property>

	<property>
		<name>dfs.namenode.shared.edits.dir</name>
		<value>qjournal://10.13.4.56:8485;10.77.113.88:8485;10.77.113.92:8485;10.77.113.96:8485;10.77.113.98:8485;/ficusMars</value>
	</property>

	<property>
		<name>dfs.journalnode.edits.dir</name>
		<value>/data1/hadoop/journaldata</value>
	</property>

	<property>
		<name>dfs.ha.automatic-failover.enabled</name>
		<value>true</value>
	</property>

	<property>
		<name>dfs.client.failover.proxy.provider.ficusMars</name>
		<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>

	<property>
		<name>dfs.ha.fencing.methods</name>
		<value>sshfence(hadoop:26387)</value>
		<final>true</final>
	</property>

	<property>
		<name>dfs.ha.fencing.ssh.private-key-files</name>
		<value>/usr/home/hadoop/.ssh/id_rsa</value>
	</property>

	<property>
		<name>dfs.ha.fencing.ssh.connect-timeout</name>
		<value>10000</value>
	</property>

	<property>
		<name>ha.zookeeper.quorum</name>
		<value>zk1.mars.grid.sina.com.cn,zk2.mars.grid.sina.com.cn,zk3.mars.grid.sina.com.cn,zk4.mars.grid.sina.com.cn,zk5.mars.grid.sina.com.cn</value> 
		<final>true</final>
	</property>

	<property>
		<name>dfs.namenode.name.dir</name>
		<value>/data1/hadoop/namenode</value>
		<final>true</final>
	</property>

	<property>
		<name>dfs.blocksize</name>
		<value>134217728</value>
	</property>

	<property>
		<name>dfs.replication</name>
		<value>2</value>
	</property>

	<property>
		<name>dfs.namenode.handler.count</name>
		<value>128</value>
	</property>

	<property>
		<name>dfs.datanode.data.dir</name>
		<value>/data1/hadoop/data/datanode,/data2/hadoop/data/datanode,/data3/hadoop/data/datanode,/data4/hadoop/data/datanode,/data5/hadoop/data/datanode,/data6/hadoop/data/datanode,/data7/hadoop/data/datanode,/data8/hadoop/data/datanode,/data9/hadoop/data/datanode,/data10/hadoop/data/datanode,/data11/hadoop/data/datanode</value>
	</property>
 
	<property>
		<name>dfs.datanode.fsdataset.volume.choosing.policy</name>
		<value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
	</property>

	<property>
		<name>dfs.datanode.failed.volumes.tolerated</name>
		<value>3</value>
	</property>

	<property>
		<name>dfs.datanode.du.reserved</name>
		<value>32212254720</value>
	</property>

	<property>
		<name>dfs.datanode.balance.bandwidthPerSec</name>
		<value>52428800</value>
	</property>

	<property>
		<name>dfs.datanode.max.xcievers</name>
		<value>8192</value>
	</property>

	<property>
		<name>dfs.datanode.handler.count</name>
		<value>32</value>
	</property>

	<property>
		<name>dfs.client.read.shortcircuit</name>
		<value>true</value>
	</property>

	<property>
		<name>dfs.domain.socket.path</name>
		<value>/var/run/hadoop-hdfs/dn_socket</value>
	</property>

	<property>
		<name>dfs.namenode.datanode.registration.ip-hostname-check</name>
		<value>fasle</value>
	</property>

	<!-- Default for hdfs acl
  <property>
    <name>dfs.permissions.superusergroup</name>
    <value>supergroup</value>
  </property>
  -->
	
	<property>
		<name>dfs.permissions.enabled</name>
		<value>true</value>
	</property>

	<property>
		<name>dfs.namenode.acls.enabled</name>
		<value>true</value>
	</property>
</configuration>
